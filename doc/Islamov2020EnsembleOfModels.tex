\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\newcommand{\hdir}{.}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{graphicx}
\newcommand{\real}{\mathbb{R}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\integ}{\mathbb{Z}}
\usepackage{bm}
\usepackage{multicol}
\usepackage{comment}
\begin{document}

\title
    [Анализ свойств ансамбля локально аппроксимирующих моделей] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Анализ свойств ансамбля локально аппроксимирующих моделей}
\author
    [Р.\,И.~Исламов, А.\,В.~Грабовой, В.\,В.~Стрижов] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {Р.\,И.~Исламов, А.\,В.~Грабовой, В.\,В.~Стрижов} % основной список авторов, выводимый в оглавление
    [Р.\,И.~Исламов$^1$, А.\,В.~Грабовой$^1$, В.\,В.~Стрижов$^{1}$] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    {islamov.ri@phystech.edu; grabovoy.av@phystech.edu;  strijov@ccas.ru}
%\thanks
%    {Работа выполнена при
%     %частичной
%     финансовой поддержке РФФИ, проекты \No\ \No 00-00-00000 и 00-00-00001.}
\organization
    {$^1$Московский физико-технический институт}
\abstract
    {Данная работа посвящена построению универсальной модели в виде ансамбля локальных моделей. Для решения задачи регрессии  предлагается использовать многоуровневый подход. Множество объектов выборки разбивается на несколько подмножеств, каждому подмножеству ставится в соответствие одна локальная модель, оптимально аппроксимирующая данное подмножество. Для аппроксимации генеральной выборки строится универсальный аппроксиматор, который представлен в виде ансамбля локальных моделей. В качестве коэффициентов шлюзовой функции используется выпуклая комбинация локальных моделей, значение которой зависит от объекта, для которого производится предсказание. Ансамблевый подход позволяет описывать те выборки, которые затруднительно описать одной моделью. Для анализа свойств проводится вычислительный эксперимент. В качестве данных используются синтетические и реальные выборки.   
	
\bigskip
\noindent
\textbf{Ключевые слова}: \emph {локальная модель; линейные модели; ансамбль моделей.}
}

\maketitle
\linenumbers

\section{Введение}

В прикладных задачах данные порождены несколькими источниками. В таких случаях качество предсказания можно повышать, используя несколько моделей. Если моделей на самом деле меньше, чем нужно, то веса лишних моделей будут малы и их вклад будет несущественен. Преимуществом ансамбля является его способность описывать те выборки, которые затруднительно описывать одной моделью.

В данной работе исследуется проблема построения ансамбля локальных моделей. \textit{Локальная модель} --- модель, которая аппроксимирует объекты, находящиеся в одной связной области в пространстве объектов. В качестве решающей функции используется выпуклая комбинация локальных моделей, при этом веса локальных моделей не постоянны, а зависят от положения объекта в пространстве объектов. 

В данной работе предполагается, что вклад каждой локальной модели в целевую переменную зависит от рассматриваемого объекта. Ансамбль локальных моделей использует шлюзовую функцию, которая определяет значимость предсказания каждой локальной модели, входящей в ансамбль.

В данной работе каждая локальная модель является линейной. В качестве функции ошибки используется логарифм правдоподобия модели. Оптимальные параметры ансамбля и локальных моделей находятся при решении двухуровневой задачи оптимизации. На первом шаге оптимизируются параметры локальных моделей при фиксированных параметрах шлюзовой функции, на втором шаге оптимизируется параметры шлюзовой при найденных фиксированных параметрах локальных моделей. 

Алгоритмы тестировались на синтетических и реальных данных.  Эксперименты показали преимущество использования ансамбля локальных моделей по сравнению с использованием одной модели.

Ансамблевый подход стал предметом многих исследований. Ансамбль моделей использовался в работах\cite{Yumlu2003}, \cite{Cheung1995}, \cite{Weigend2000}. В работе \cite{Yuksel2012} представлен обзор методов и моделей в задачах ансамбля моделей. В данной работе представлены виды шлюзовых функций. Приведен анализ разных моделей, которые могут выступать в качестве локальной модели. 

Ансамблевый подход имеет множество приложений в прикладных задачах. В работе \cite{article} предложен метод распознавания рукописных цифр. Метод распознания текстов при помощи ансамбля локальных моделей исследуется в работах \cite{Estabrooks2001}, а для распознавания речи --- в \cite{Mossavat2010}, \cite{Peng1996}. В работе \cite{Sminchisescu2007} исследуется смесь экспертов для задачи распознавания трехмерных движений человека.


Были предложены различные типы локальных моделей, такие как SVM \cite{Collobert2002}, Гауссовский процесс \cite{Tresp01mixturesof}  и нейронные сети \cite{Shazeer2017}. Другие работы была сосредоточены на различных конфигурациях, таких как иерархическая структура \cite{NIPS1991_514}, бесконечное число экспертов \cite{Rasmussen} и последовательное добавление экспертов \cite{Aljundi2016}. \cite{garmash-monz-2016-ensemble} предлагает модель ансамбля локальных моделей для машинного перевода. Стробирующая сеть обучается на предварительно обученной модели NMT ансамбля. 



\section{Постановка задачи построения ансамбля локальных моделей}

Задано множество объектов $\Omega$, признаки которых описываются матрицей
\[\mathbf{X} \in \real^{N\times n}, \eqno(2.1)\]
где $N$ --- число объектов во множестве, а $n$ --- размерность признакового пространства. Каждому объекту $\omega_i$ из $\Omega$ соответствует признаковое описание $\mathbf{x}_i \in \real^n$, которое является $i$-ой строкой матрицы $\mathbf{X}$, и значение целевой переменной $y_i \in \real$.  Введем выборку данных $\mathfrak{D}$:
\[\mathfrak{D} = \{(\mathbf{x}_i, y_i)~|~ i \in \overline{1, N}\}. \eqno(2.2)\]

В данной работе предполагается, что выборка $\mathfrak{D}$ порождена $K$ источниками. Это предположение индуцирует разбиение множества индексов $I = \{1, 2, \dotsc, N\}$ на $K$ непересекающихся подмножеств $I_k$:

\[I = \bigcup\limits_{k=1}^K I_k. \eqno(2.3)\]
Разбиение индексного множества $I$ индуцирует разбиение множества объектов $\Omega$ на подмножества $\Omega_k$
\[\Omega = \bigcup\limits_{k=1}^K \Omega_k, \qquad \Omega_k = \{\omega_i \in \Omega~|~i \in I_k\}\eqno(2.4)\]
и выборку $\mathfrak{D}$ на подвыборки $\mathfrak{D}_k$
\[\mathfrak{D} = \bigcup\limits_{k=1}^K \mathfrak{D}_k, \qquad \mathfrak{D}_k = \{(\mathbf{x}_i, y_i) \in \mathfrak{D}~|~i \in I_k\}\eqno(2.5)\]
Для каждого подмножества объектов $\Omega_k$ используется своя локальная модель.\\
\begin{Definition}
\label{def:1}
Модель $\mathbf{g}_k$ называется локальной, если она аппроксимирует аппроксимирует подвыборку $\mathfrak{D}_k = \{(\mathbf{x}_i, y_i) \in \mathfrak{D}~|~i \in I_k\}$.
\end{Definition}
В данной работе локальный модели объединены в ансамбль локальных моделей.\\
\begin{Definition}
\label{def:2}
Ансамбль локальных моделей --- мультимодель, определяющая правдоподобие веса $\pi_k$ каждой локальной модели $\textbf{g}_k$ на признаковом описании объекта $\textbf{x}$.
\[\mathbf{f} = \sum\limits_{k=1}^K\pi_k\mathbf{g}_k(\mathbf{x},\mathbf{w}_k),\qquad \pi_k\left(\mathbf{x}, \mathbf{V}\right): \real^{n\times |\mathbf{V}|} \rightarrow [0,1], \qquad \sum\limits_{k=1}^K\pi_k\left(\mathbf{x}, \mathbf{V}\right) = 1, \eqno(2.6)\]
где $\mathbf{f}$ --- ансамбль локальных моделей, $\mathbf{g}_k$ --- локальная модель, $\pi_k$ --- шлюзовая функция, $\mathbf{V}$--- параметры шлюзовой функции. 
\end{Definition}



В работе в качестве локальной модели $\mathbf{g}_k$ используется линейная модель, а в качестве шлюзовой функции $\bm{\pi}$ используется двухслойная нейросеть:

\[\mathbf{g}_k(\mathbf{x}, \mathbf{w}_k) = \mathbf{w}_k^{\mathsf{T}}\mathbf{x}, \qquad \bm{\pi}\left(\mathbf{x}, \mathbf{V}\right) = \text{softmax}\bigg(\mathbf{V}_1^{\mathsf{T}}\sigma\left(\mathbf{V}_2^{\mathsf{T}}\mathbf{x}\right)\bigg), \eqno(2.7)\]
где $\mathbf{V} = \{\mathbf{V}_1, \mathbf{V}_2\}$ --- параметры шлюзовой функции, $\sigma(x)$ --- сигмоидная функция. Введем понятие расстояние между двумя объектами.\\
\begin{Definition}
\label{def:3}
Расстоянием между двумя объектами $\omega_1$ и $\omega_2$ из $\Omega$ называется число, равное расстоянию между векторами $\mathbf{x}_1, \mathbf{x}_2$ признаковых описаний этих объектов:

\[\rho(\omega_1, \omega_2) = ||\mathbf{x}_1 - \mathbf{x}_2||_2. \eqno(2.8) \] 
\end{Definition} 

Будем считать, что каждый объект имеет признаковое описание $\mathbf{x}$, взятый из некоторого вероятностного распределения. Пусть этому распределению соответствует вероятностная мера $\mathsf{F}(x)$ в пространстве признаков. Тогда пространство локальных моделей является гильбертовым пространством, в котором введено скалярное произведение.\\
\begin{Definition}
\label{def:4}
Скалярным произведением между двумя локальными моделями $\mathbf{g}_i$ и $\mathbf{g}_j$ называется число, вычисляемое по формуле
\[ \left<\mathbf{g}_i, \mathbf{g}_j\right> = \mathsf{cov}\bigg(\mathbf{g}_i(\mathbf{x},\mathbf{w}_i),\mathbf{g}_j(\mathbf{x}, \mathbf{w}_j) \bigg) = \mathsf{E}\bigg(\stackrel{\mathsf{o}}{\mathbf{g}}_i(\mathbf{x}\cdot \mathbf{w}_i)\cdot\stackrel{\mathsf{o}}{\mathbf{g}}_j(\mathbf{x}, \mathbf{w}_j) \bigg), \eqno(2.9)\]
где $\mathsf{E}\left(\stackrel{\mathsf{o}}{\xi}\cdot \stackrel{\mathsf{o}}{\eta}\right) = \int\limits_{\real^n}\stackrel{\mathsf{o}}{\xi}(\mathbf{x})\stackrel{\mathsf{o}}{\eta}(\mathbf{x})\mathsf{dF}(\mathbf{x})$ --- математическое ожидание произведения центрированных случайных величин $\xi(\mathbf{x})$ и $\eta(\mathbf{x}$). 
\end{Definition}
Данное определение можно интерпретировать следующим образом: чем ближе скалярное произведение к нулю, тем дальше локальные модели друг от друга в пространстве моделей, и наоборот.


Для нахождения оптимальных параметров мультимодели используется функция ошибки следующего вида:
\[\mathcal{L}\left(\textbf{V}, \textbf{W}\right) = \sum\limits_{(\textbf{x}, y) \in \mathcal{D}} \sum\limits_{k=1}^K\pi_k\left(\textbf{x}, \textbf{V}\right)\left(y - \textbf{w}_k^{\mathsf{T}}\textbf{x}\right)^2 + R\left(\mathbf{V}, \mathbf{W}\right), \eqno(2.10)\] 
где $\mathbf{W} = [\mathbf{w}_1, \mathbf{w}_2, \dotsc, \mathbf{w}_k]$ --- параметры локальных моделей, $R\left(\mathbf{V}, \mathbf{W}\right)$ --- регуляризация параметров. Оптимальные параметры определяются из выражения

\[\hat{\mathbf{V}}, \hat{\mathbf{W}} = \argmin\limits_{\mathbf{V}, \mathbf{W}}{\mathcal{L}\left(\textbf{V}, \textbf{W}\right)}. \eqno(2.11)\]

В качестве базового алгоритма используется ЕМ-алгоритм. Алгоритм подробно описан в \cite{Bishop2011}. Формулы, по которым происходит оптимизация функции ошибки $(2.11)$, приведены в работе \cite{Grabovoy2020}. 

\section{Вычислительный эксперимент}

\subsection{Постановка проблемы}

Данный эксперимент ставится для того, чтобы показать, что одна линейная модель плохо аппроксимирует выборку, объекты которой порождены несколькими источниками.

В качестве данных используются синтетическая выборка. Рассмотрим две подвыборки объектов, имеющих по одному признаку $x^k$, описываемых линейной моделью с нормальным шумом:
\[\mathbf{y}_k = \alpha_{k}\mathbf{x}^k+ \bm\varepsilon, \qquad k \in \{1,2\},  \qquad x_k,y_k \in \real,\qquad \varepsilon \in \mathcal{N}(0, 1). \eqno(3.1)\] 
В качестве общей выборки рассматривается конкатенация двух подвыборок, описываемая вектором целевой переменной $\mathbf{y}$ и матрицей признаков $\mathbf{X}$:	
\[\mathbf{y} = \begin{pmatrix}
\mathbf{y}_1\\
\mathbf{y}_2
\end{pmatrix}, \qquad\mathbf{X} = \begin{pmatrix}
\mathbf{x}^1 & \mathbf{0}\\
\mathbf{0} & \mathbf{x}^2
\end{pmatrix}.\eqno(3.2)\]

На общей выборке $\mathbf{X}$ обучается линейная модель. Линейная модель хорошо аппроксимирует данную выборку (см. рис. $1\text{a}$). 

Во втором эксперименте две подвыборки сливаются в одну общую выборку, описываемая целевой переменной $\mathbf{y}$ и матрицей признаков $\hat{\mathbf{X}}$:
\[\mathbf{y} = \begin{pmatrix}
\mathbf{y}_1\\
\mathbf{y}_2
\end{pmatrix}, \qquad \mathbf{\hat{X}} = \begin{pmatrix}
\mathbf{x}^1 & \bm\varepsilon_1\\
\bm\varepsilon_2 & \mathbf{x}^2
\end{pmatrix},\eqno(3.3)\]
где $\bm\varepsilon_1, \bm\varepsilon_2 \in \mathcal{N}(0,1)$. На построенной модели также обучается линейная модель. В этом случае линейная модель плохо аппроксимирует данную выборку (см. рис. $1\text{б}$). 




\begin{figure}[h]
\begin{center}
\begin{minipage}[h]{0.49\linewidth}
\begin{center}\includegraphics[width=1.2\linewidth]{experiment1-zeros.pdf}  а) \end{center}
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\begin{center}\includegraphics[width=1.2\linewidth]{experiment1-random.pdf}  б) \end{center}
\end{minipage}
\caption{а) Признаки, соответствующие другому подмножеству, заполнялись нулями, б) признаки, соответствующие другому подмножеству, заполнялись случайными числами. Точки соответствуют правильным ответам, плоскости задают предсказание линейной модели для каждого из подмножеств.}
\label{ris:image1}
\end{center}
\end{figure}


Данный эксперимент показывает, что для аппроксимации выборки, порожденной несколькими источниками, одна модель не подходит. Таким образом, при построение модели для обучения нужно учитывать гипотезу порождения данных. Нередко оказывается, что данные порождены несколькими источниками. В этом случае для лучшей аппроксимации можно использовать ансамбль локальных моделей, где каждая локальная модель обрабатывает свою область признакового пространства (в одной области объекты имеют схожие признаки, объекты из разных областей имеют разные признаковые описания).

\subsection{Решение проблемы при помощи ансамбля локальных моделей}

Данный эксперимент ставится для того, чтобы показать, что ансамбль локальных моделей хорошо аппроксимирует выборку, порожденную несколькими источниками.


В данном эксперименте для аппроксимации используется ансамбль двух линейных локальных моделей. 

В первом эксперименте ансамбль обучается на выборке $(\mathbf{y}, \mathbf{X})$, а во втором --- на выборке $(\mathbf{y}, \mathbf{\hat{X}})$. Ансамбль двух линейных локальных моделей хорошо аппроксимирует выборку (см. рис. $2$ а, б).  


Данный эксперимент показывает, что для аппроксимации выборки, порожденной несколькими источниками, подходит ансамбль локальных моделей. Качество аппроксимации ансамбля моделей выше, чем при использовании лишь одной модели. 


\newpage
\begin{figure}[h]
\begin{center}
\begin{minipage}[h]{0.49\linewidth}
\begin{center}\includegraphics[width=1.2\linewidth]{experiment2-zeros.pdf}  а) \end{center}
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\begin{center}\includegraphics[width=1.2\linewidth]{experiment2-random.pdf}  б) \end{center}
\end{minipage}
\caption{а) Признаки, соответствующие другому подмножеству, заполнялись нулями, б) признаки, соответствующие другому подмножеству, заполнялись случайными числами. Точки соответствуют правильным ответам, плоскости задают предсказание мультимодели для каждого из подмножеств.}
\label{ris:image1}
\end{center}
\end{figure}



\subsection{Анализ ансамбля моделей в зависимости от уровня шума}

{Синтетические данные.} В качестве данных используются синтетические данные. Используется две подвыборки, каждая из которых описывается линейной моделью с нормальным шумом:
\[\mathbf{y}_k = \alpha_{k}\mathbf{x}^k+ \bm\varepsilon, \qquad k \in \{1,2\},  \qquad x_k,y_k \in \real,\qquad \varepsilon \in \mathcal{N}(0, 1). \eqno(3.4)\] 

В качестве общей выборки рассматривается конкатенация двух подвыборок, описываемая вектором целевой переменной $\mathbf{y}$ и матрицей признаков $\mathbf{\hat{X}}$:	
\[\mathbf{y} = \begin{pmatrix}
\mathbf{y}_1\\
\mathbf{y}_2
\end{pmatrix},\qquad  \mathbf{\hat{X}} = \begin{pmatrix}
\mathbf{x}^1 & \bm\varepsilon_1\\
\bm\varepsilon_2 & \mathbf{x}^2
\end{pmatrix},\eqno(3.5)\]
где $\bm\varepsilon_1, \bm\varepsilon_2 \in \mathcal{N}(0,\sigma)$. На общей выборке обучается ансамбль из двух локальных моделей, каждая из которых является линейной. Исследуется зависимость введенного расстояния $(2.9)$ от параметра шума $\sigma$. График представлен на рисунке $3$:

\begin{figure}[h]
\center{\includegraphics[width=0.7\linewidth]{Pearson_cor_sigma_synthetic.pdf}}
\caption{График зависимости расстояние между локальными моделями от параметра шума $\sigma$ для синтетических данных.}
\label{ris:image}
\end{figure}


На графике видно, что при малом параметре шума $\sigma$ (меньшем, чем пороговое значение) локальные модели близки, корреляция между ними приблизительно равна единице. Это означает, что при малом параметре $\sigma$ шум практически не влияет на данные, для аппроксимации выборки достаточно одной линейной модели, поэтому параметры двух локальных моделей становятся приблизительно одинаковыми. 

На графика также видно, что при параметре шума $\sigma$ большем, чем пороговое значение, локальные модели становятся независимыми друг от друга, корреляция между ними приблизительно равна нулю. Это означает, что шумовые признаки сказываются на данных и для аппроксимации выборки необходимы две модели.

\textbf{Данные на основе датасэтов Boston housing и Servo.} Выборки Boston housing и Servo описываются матрицами признаков $\mathbf{X}_{b} \in \real^{506\times 13}$ и $\mathbf{X}_s\in \real^{167\times 4}$, а также векторами целевой переменной $\mathbf{y}_b \in \real^{506}$ и $\mathbf{y}_s \in \real^{167}$. В качестве общей выборки рассматривается конкатенация выборок Boston housing и Servo, описываемая вектором целевой переменной $\tilde{\mathbf{y}}$ и $\tilde{\mathbf{X}}$:

\[\tilde{\mathbf{y}} = \begin{pmatrix}
\mathbf{y}_b\\
\mathbf{y}_s
\end{pmatrix} \in \real^{673}, \qquad \tilde{\mathbf{X}} = \begin{pmatrix}
\mathbf{X}_b\\
\mathbf{X}_s ~~\mathbf{\mathcal{E}}
\end{pmatrix} \in \real^{673\times 13},\eqno(3.6)\]
где $\mathbf{\mathcal{E}}$ --- матрица размера $167\times 9$, каждый элемент которой из $\mathcal{N}(0,\sigma)$. На общей выборке обучается ансамбль из двух локальных моделей, каждая из которой является линейной. Исследуется зависимость расстояния между локальными моделями от параметра шума $\sigma$. График представлен на рисунке $4$:


\begin{figure}[h]
\center{\includegraphics[width=0.7\linewidth]{Pearson_cor_sigma_boston_and_servo(1).pdf}}
\caption{График зависимости расстояние между локальными моделями от параметра шума $\sigma$ для выборок Boston housing и Servo.}
\label{ris:image}
\end{figure}

На графике видно, что при увеличении параметра шума $\sigma$ есть тенденция к уменьшению скалярного произведения между моделями. Это означает,  с увеличением шума локальные модели отдаляются друг от друга.

Исследуем качество аппроксимации при помощи ансамбля двух линейных локальных моделей. Ошибка аппроксимации вычисляется по формуле

\[error = \sum_{k=1}^2\sum_{i=1}^N \pi_k(\mathbf{x}_i, \mathbf{w}^k)(y^{pred_k}_i - y^{real}_i)^2,\eqno(3.7)\]
где $N = 673$ --- количество объектов в общей выборке, $y_i^{pred_k}$ --- предсказанное значение для $i$-го объекта $k$-ой моделью, а $y^{real}_i$ --- значение целевой переменной $i$-го объекта.  График зависимости ошибки аппроксимации от параметра шума представлен на рисунке $5$:

\begin{figure}[h]
\center{\includegraphics[width=0.7\linewidth]{Errors_sigma_boston_and_servo.pdf}}
\caption{График зависимости ошибки аппроксимации от параметра шума $\sigma$ для выборок Boston housing и Servo.}
\label{ris:image}
\end{figure}
\bibliographystyle{unsrt}
\bibliography{Islamov}

На графике видно, что с ростом параметра шума $\sigma$ качество аппроксимации уменьшается. 

\end{document}
